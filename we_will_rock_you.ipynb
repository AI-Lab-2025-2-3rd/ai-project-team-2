{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI-Lab-2025-2-3rd/ai-project-team-2/blob/main/we_will_rock_you.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d4P39vAhbpmi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "import zipfile\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_path = \"/content/drive/Shareddrives/we_will_rock_you/rock_data.zip\"\n",
        "extract_path = \"/content/rolling_stones_data\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    file_list = zip_ref.namelist()\n",
        "    for file in tqdm(file_list, desc=\"Extracting\"):\n",
        "        zip_ref.extract(file, extract_path)\n",
        "\n",
        "TRAIN_DIR = \"/content/rolling_stones_data/open/train\"\n",
        "TEST_DIR = \"/content/rolling_stones_data/open/test\"\n",
        "\n",
        "img_size = 224\n",
        "batch_size = 32\n",
        "val_split = 0.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYPDXrG5UG8v",
        "outputId": "9e87eff0-bfb3-48de-932a-0b158ae95de2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting: 100%|██████████| 475038/475038 [04:18<00:00, 1835.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    validation_split=val_split\n",
        ")\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    target_size=(img_size, img_size),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_gen = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    target_size=(img_size, img_size),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "num_classes = train_gen.num_classes\n",
        "class_indices = train_gen.class_indices\n",
        "print(\"Classes:\", class_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8owUFt9BVNTQ",
        "outputId": "4d6a995b-0d82-49bb-e8e1-46741d55b0a6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 304019 images belonging to 7 classes.\n",
            "Found 76001 images belonging to 7 classes.\n",
            "Classes: {'Andesite': 0, 'Basalt': 1, 'Etc': 2, 'Gneiss': 3, 'Granite': 4, 'Mud_Sandstone': 5, 'Weathered_Rock': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = {cls: len(os.listdir(os.path.join(TRAIN_DIR, cls)))\n",
        "                for cls in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, cls))}\n",
        "\n",
        "classes = list(class_counts.keys())\n",
        "counts = list(class_counts.values())\n",
        "class_labels = np.arange(len(classes))\n",
        "\n",
        "weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=class_labels,\n",
        "    y=np.concatenate([np.full(count, i) for i, count in enumerate(counts)])\n",
        ")\n",
        "class_weight = {i: w for i, w in enumerate(weights)}\n",
        "print(\"class_weight:\", class_weight)"
      ],
      "metadata": {
        "id": "0mP1TLrebz-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "456ebf57-983f-47e4-cd1d-e92900f3e5ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class_weight: {0: np.float64(2.024937390099643), 1: np.float64(1.2394085071131782), 2: np.float64(0.5842317986781682), 3: np.float64(1.4605873558226325), 4: np.float64(0.6067999533746681), 5: np.float64(0.7344829319015536), 6: np.float64(3.4068761486395625)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = list(class_counts.keys())\n",
        "counts = list(class_counts.values())\n",
        "class_labels = np.arange(len(classes))\n",
        "\n",
        "weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=class_labels,\n",
        "    y=np.concatenate([np.full(count, i) for i, count in enumerate(counts)])\n",
        ")\n",
        "\n",
        "class_weight = {i: w for i, w in enumerate(weights)}\n",
        "print(\"클래스 인덱스:\", train_gen.class_indices)\n",
        "print(\"class_weight:\", class_weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfbKK-FgYYaa",
        "outputId": "ea35c5f7-7edd-4eca-adcf-2a936647e6d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "클래스 인덱스: {'Andesite': 0, 'Basalt': 1, 'Etc': 2, 'Gneiss': 3, 'Granite': 4, 'Mud_Sandstone': 5, 'Weathered_Rock': 6}\n",
            "class_weight: {0: np.float64(2.024937390099643), 1: np.float64(1.2394085071131782), 2: np.float64(0.5842317986781682), 3: np.float64(1.4605873558226325), 4: np.float64(0.6067999533746681), 5: np.float64(0.7344829319015536), 6: np.float64(3.4068761486395625)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingMonitor(callbacks.Callback):\n",
        "    def __init__(self, val_data, val_steps):\n",
        "        super().__init__()\n",
        "        self.val_data = val_data\n",
        "        self.val_steps = val_steps\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "        for _ in range(self.val_steps):\n",
        "            x_batch, y_batch = next(self.val_data)\n",
        "            preds = self.model.predict(x_batch, verbose=0)\n",
        "            val_preds.append(np.argmax(preds, axis=1))\n",
        "            val_labels.append(np.argmax(y_batch, axis=1))\n",
        "        y_true = np.concatenate(val_labels)\n",
        "        y_pred = np.concatenate(val_preds)\n",
        "\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        FP = cm.sum(axis=0) - np.diag(cm)\n",
        "        TN = cm.sum() - (cm.sum(axis=1) + cm.sum(axis=0) - np.diag(cm))\n",
        "        fpr_per_class = FP / (FP + TN + 1e-7)\n",
        "        avg_fpr = np.mean(fpr_per_class)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} - val_accuracy: {logs.get('val_accuracy'):.4f}, avg_val_FPR: {avg_fpr:.4f}\")\n"
      ],
      "metadata": {
        "id": "E7vaQb6dYRg0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = EfficientNetB0(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(img_size, img_size, 3)\n",
        ")\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = layers.Input(shape=(img_size, img_size, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = models.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEXIkL0cYskP",
        "outputId": "14150234-4a0a-4827-c9ba-58182f6a85fa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_epochs = 5\n",
        "fine_tune_epochs = 5\n",
        "total_epochs = initial_epochs + fine_tune_epochs\n",
        "\n",
        "# 20%만 학습\n",
        "subset_ratio = 0.2\n",
        "steps_per_epoch = max(1, int(len(train_gen) * subset_ratio))\n",
        "validation_steps = max(1, int(len(val_gen) * subset_ratio))\n",
        "\n",
        "monitor_callback = TrainingMonitor(val_gen, validation_steps)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history_initial = model.fit(\n",
        "    train_gen,\n",
        "    epochs=initial_epochs,\n",
        "    validation_data=val_gen,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        "    class_weight=class_weight,\n",
        "    callbacks=[monitor_callback]\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSePOvBdZSRF",
        "outputId": "74cf4215-8725-42ea-a8b7-d92655a22442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1900/1900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 431ms/step - accuracy: 0.5295 - loss: 1.4721Epoch 1 - val_accuracy: 0.4938, avg_val_FPR: 0.0921\n",
            "\u001b[1m1900/1900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1289s\u001b[0m 663ms/step - accuracy: 0.5295 - loss: 1.4720 - val_accuracy: 0.4938 - val_loss: 1.3928\n",
            "Epoch 2/5\n",
            "\u001b[1m1477/1900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m3:00\u001b[0m 427ms/step - accuracy: 0.6314 - loss: 1.1828"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.trainable = True\n",
        "fine_tune_at = int(len(base_model.layers) * 0.8)\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history_fine = model.fit(\n",
        "    train_gen,\n",
        "    epochs=total_epochs,\n",
        "    initial_epoch=history_initial.epoch[-1] + 1,\n",
        "    validation_data=val_gen,\n",
        "    steps_per_epoch=len(train_gen),\n",
        "    validation_steps=len(val_gen),\n",
        "    class_weight=class_weight,\n",
        "    callbacks=[monitor_callback]\n",
        ")"
      ],
      "metadata": {
        "id": "2A-rtbRwalK_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}